"""NSML Example (Autogenerated name.)
=====================================


Please edit this text to describe what this file does. It is included
automatically in the documentation.

"""

import argparse
import os
import pickle

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from PIL import Image
from torch.autograd import Variable
from torch.utils.data import DataLoader
from torchvision import transforms
from torchvision.datasets.mnist import MNIST, read_image_file, read_label_file

try:
    import nsml
    from nsml import IS_DATASET, DATASET_PATH
except:
    pass

from dropout import ContiguousDropout

def infer(input, top_k=100):
    # load data into torch tensor
    model.eval()
    # from list to tensor
    image = torch.stack(preprocess(None, input))
    image = Variable(image.cuda())
    clean_state, _ = model(image)
    batch_size, all_cls = clean_state.size()
    # prediction format: ([torch.Tensor], [toch.Tensor)
    prediction = F.softmax(clean_state).topk(min(top_k, all_cls))
    # output format
    # [[(key, prob), (key, prob)... ], ...]
    return list(zip(list(prediction[0].data.cpu().squeeze().tolist()),
                    list(prediction[1].data.cpu().squeeze().tolist())))


def _normalize_image(image_tensor, label, transform):
    new_tensor = []
    for idx, image in enumerate(image_tensor):
        if torch.is_tensor(image):  # else, image is instance of PIL.Image
            image = Image.fromarray(image.numpy(), mode='L')
        if label is not None:
            new_tensor.append([transform(image), label[idx]])
        else:
            new_tensor.append(transform(image))
    return new_tensor


def preprocess(output_path, data):
    transform = transforms.Compose([transforms.ToTensor(),
                                    transforms.Normalize((0.1307,), (0.3081,))])
    # if-statement for evalaute/infer
    if output_path:
        data_set = {
            'train': _normalize_image(data['train']['data'], data['train']['label'], transform),
            'test': _normalize_image(data['test']['data'], data['test']['label'], transform)
        }
        with open(output_path[0], 'wb') as file:
            torch.save(data_set, file)
    else:
        data_set = _normalize_image(data['data'], data['label'], transform)
        return data_set


# supported by dataset owner
def data_loader(root_path, train=True):
    if train:
        data_dict = {
            'train': {
                'data': read_image_file(os.path.join(root_path, 'train', 'train-images-idx3-ubyte')),
                'label': read_label_file(os.path.join(root_path, 'train', 'train-labels-idx1-ubyte'))},
            'test': {
                'data': read_image_file(os.path.join(root_path, 'test', 't10k-images-idx3-ubyte')),
                'label': read_label_file(os.path.join(root_path, 'test', 't10k-labels-idx1-ubyte'))

            }
        }
    else:
        data_dict = {
            'data': read_image_file(os.path.join(root_path, 'test', 't10k-images-idx3-ubyte')),
            'label': None
        }
    return data_dict

# simple CNN
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)
        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)
        self.dropout2d = nn.Dropout2d(args.dropoutprob)
        self.cdropout = ContiguousDropout(args.dropoutprob,dropout_dim =1)
        self.fc1 = nn.Linear(320, 50)
        self.fc1_bn = nn.BatchNorm1d(50)
        self.dropout = nn.Dropout(args.dropoutprob)
        self.fc2 = nn.Linear(50, 10)
        self.conv2_bn = nn.BatchNorm2d(20)

    def forward(self, x,n_used1=None,n_used2=None):
        x = F.relu(F.max_pool2d(self.conv1(x), 2))
        x = self.conv2_bn(self.conv2(x))
        x = self.cdropout(x,dropout_start = n_used1) if (args.cdropout or n_used1 is not None) else self.dropout2d(x)
        if not args.cdropout and n_used1 is not None:
            x = x*(x.shape[1]/max(1,n_used1)) # DEBUG
        x = F.relu(F.max_pool2d(x, 2))
        x = x.view(-1, 320)
        x = F.relu(self.fc1_bn(self.fc1(x)))
        x = self.cdropout(x,dropout_start = n_used2) if (args.cdropout or n_used2 is not None) else self.dropout(x)
        if not args.cdropout and n_used2 is not None: # DEBUG
            x = x*(x.shape[1]/max(1,n_used2))
        x = self.fc2(x)
        return F.log_softmax(x)


def train(epoch, scope):
    model.train()
    full_data_size = len(train_loader.dataset)
    batch_size = full_data_size // args.batch_size
    residual = full_data_size % args.batch_size
    if residual != 0:
        batch_size += 1

    for batch_idx, (data, target) in enumerate(train_loader):
        target_data = target
        if args.cuda:
            data, target = data.cuda(), target.cuda()
        data, target = Variable(data), Variable(target)
        optimizer.zero_grad()
        output = model(data)
        loss = F.nll_loss(output, target)

        loss.backward()
        optimizer.step()
        if batch_idx % args.log_interval == 0:
            # nsml.report(
            #     epoch=epoch,
            #     epoch_total=args.epochs+1,
            #     iter=batch_idx,
            #     iter_total=batch_size,
            #     train__loss=loss.data[0],
            #     step=(epoch-1)*batch_size+batch_idx,
            #     scope=dict(scope, **locals())
            # )
            print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
                epoch, batch_idx * len(data), len(train_loader.dataset),
                100. * batch_idx / len(train_loader), loss.data[0]))
            train_losses.append([epoch,batch_idx,loss.cpu().data.numpy()]) # DEBUG

def evaluate_test(scope,**kwargs):
    """Returns scores on test set. 
    """
    model.eval()
    test_loss = 0
    correct = 0
    n_samples = len(test_loader.dataset)

    for data, target in test_loader:
        if args.cuda:
            data, target = data.cuda(), target.cuda()
        data, target = Variable(data, volatile=True), Variable(target)
        output = model(data,**kwargs)
        test_loss += F.nll_loss(output, target, size_average=False).data[0]  # sum up batch loss
        pred = output.data.max(1)[1]  # get the index of the max log-probability
        correct += pred.eq(target.data).cpu().float().sum()
    test_loss /= n_samples
    accuracy = correct / n_samples 
    return test_loss,accuracy,correct,n_samples

def test(scope):
    test_loss,accuracy,correct,n_samples = evaluate_test(scope=locals())
    print('\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n'.format(
        test_loss, correct, n_samples,
        100. * accuracy))
    # nsml.report(
    #     summary=True,
    #     epoch=epoch,
    #     test__loss=test_loss,
    #     test__accuracy=accuracy
    # )
    test_losses.append([epoch,test_loss.data.cpu().numpy()]) # DEBUG

def eval_incrementally_dropout(scope):
    model.eval()
    everyth = 1
    results = torch.zeros(20//everyth,50//everyth,2)
    test_loss,accuracy,_,_ = evaluate_test(scope=locals(),n_used1=0,n_used2=0)
    results[0,:,0] = results[:,0,0] = test_loss
    results[0,:,1] = results[:,0,1] = accuracy

    for k1 in range(20//everyth):
        for k2 in range(50//everyth):
            test_loss,accuracy,_,_ = evaluate_test(scope=locals(),n_used1=k1*everyth+1,n_used2=k2*everyth+1)
            results[k1,k2,0] = test_loss
            results[k1,k2,1] = accuracy
            print(k1,k2,'',test_loss.cpu().numpy(),accuracy.cpu().numpy())
    return results

def save_pickle(object,filename):
    with open(filename,'wb') as f:
            pickle.dump(object,f)

def load_pickle(filename):
    with open(filename,'rb') as f:
            return pickle.load(f)

if __name__ == '__main__':
    # Training settings
    parser = argparse.ArgumentParser(description='PyTorch MNIST Example')
    parser.add_argument('--mode', type=str, default='train')
    parser.add_argument('--batch-size', type=int, default=64, metavar='N',
                        help='input batch size for training (default: 64)')
    parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',
                        help='input batch size for testing (default: 1000)')
    parser.add_argument('--epochs', type=int, default=100, metavar='N',
                        help='number of epochs to train (default: 10)')
    parser.add_argument('--lr', type=float, default=0.01, metavar='LR',
                        help='learning rate (default: 0.01)')
    parser.add_argument('--momentum', type=float, default=0.5, metavar='M',
                        help='SGD momentum (default: 0.5)')
    parser.add_argument('--no-cuda', action='store_true', default=False,
                        help='disables CUDA training')
    parser.add_argument('--seed', type=int, default=1, metavar='S',
                        help='random seed (default: 1)')
    parser.add_argument('--log-interval', type=int, default=10, metavar='N',
                        help='how many batches to wait before logging training status')
    parser.add_argument('--aux', type=float, default=1.0)
    parser.add_argument('--aux-decay', type=float, default=0.5)
    parser.add_argument('--iteration', type=str, default='0')
    parser.add_argument('--multigpu', action='store_true', default=False)
    parser.add_argument('--pause', type=int, default=0)

    ##
    parser.add_argument('--cdropout', action='store_true', default=False,
                        help='use contiguous dropout')
    parser.add_argument('--dropoutprob', type=float, default=0.5, metavar='LR',
                        help='dropout probability')
    # parser.add_argument('--fc1-size', type=int, default=320, metavar='N',help='')
    # parser.add_argument('--fc2-size', type=int, default=50, metavar='N',help='')

    args = parser.parse_args()
    args.cuda = not args.no_cuda and torch.cuda.is_available()

    torch.manual_seed(args.seed)
    if args.cuda:
        torch.cuda.manual_seed(args.seed)

    kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}
    if not IS_DATASET:
        train_loader = torch.utils.data.DataLoader(
            MNIST('/data', train=True, download=True,
                  transform=transforms.Compose([
                      transforms.ToTensor(),
                      transforms.Normalize((0.1307,), (0.3081,))
                  ])),
            batch_size=args.batch_size, shuffle=True, **kwargs)
        test_loader = torch.utils.data.DataLoader(
            MNIST('/data', train=False, transform=transforms.Compose([
                transforms.ToTensor(),
                transforms.Normalize((0.1307,), (0.3081,))
            ])),
            batch_size=args.test_batch_size, shuffle=False, **kwargs)
    else:
        preprocessed_file = ['./processed.pt']
        nsml.cache(preprocess, output_path=preprocessed_file, data=data_loader(DATASET_PATH))
        dataset = torch.load(preprocessed_file[0])
        train_loader = torch.utils.data.DataLoader(dataset['train'],
                                                   batch_size=args.batch_size, shuffle=True, **kwargs)
        test_loader = torch.utils.data.DataLoader(dataset['test'],
                                                  batch_size=args.test_batch_size, shuffle=False, **kwargs)

    model = Net()
    if args.multigpu:
        model = nn.DataParallel(model)
    if args.cuda:
        model.cuda()

    optimizer = optim.SGD(
        [
            *model.parameters()
        ],
        lr=args.lr,
        momentum=args.momentum)

    nsml.bind(infer=infer, model=model, optimizer=optimizer)
    if args.pause:
        nsml.paused(scope=locals())
    test_losses = [] # DEBUG
    train_losses = [] # DEBUG

    if args.mode == 'train':
        for epoch in range(1, args.epochs + 1):
            train(epoch, scope=locals())
            if epoch % 10 == 9:
                nsml.save(epoch)
            test(scope=locals())

    incremental_eval = eval_incrementally_dropout(scope=locals())
    torch.save(incremental_eval.cpu(), 'incremental_eval_matrix.pt') # DEBUG

    if not args.cdropout:
        # Debug : only to test if the (n/k)-factor is fair/improves.
        args.cdropout = True 
        incremental_eval = eval_incrementally_dropout(scope=locals())
        torch.save(incremental_eval.cpu(), 'incremental_eval_matrix_method2.pt') # DEBUG
        args.cdropout = False

    torch.save(model.cpu().state_dict(), 'model.pt') # DEBUG
    save_pickle(train_losses,'train_losses.pkl')
    save_pickle(test_losses,'test_losses.pkl')


    print(args) # DEBUG
    print('min train_loss: ',min([l[1] for l in train_losses]))
    print('min test_loss: ',min([l[1] for l in test_losses]))



